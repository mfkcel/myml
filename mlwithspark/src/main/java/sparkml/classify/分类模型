如果不止两类，则称为多类别分类，这时的类别一般从0开始进行标记（比如，5个类别用数字0~4表示）
分类是监督学习的一种形式，我们用带有类标记或者类输出的训练样本训练模型
（也就是通过输出结果监督被训练的模型）
每个分类或回归函数都有一个损失函数

我们将讨论Spark中常见的三种分类模型：线性模型、决策树和朴素贝叶斯模型。
------------------------------------------------------------------
线性模型
   简单而且相对容易扩展到非常大的数据集，<逻辑回归，svm>
决策树
    是一个强大的非线性技术，训练过程计算量大并且较难扩展
    （幸运的是， MLlib会替我们考虑扩展性的问题）,但是在很多情况下性能很好；

朴素贝叶斯模型
    简单、易训练，并且具有高效和并行的优点（实际中，模型训练只需要遍历所有数
    据集一次）。当采用合适的特征工程，这些模型在很多应用中都能达到不错的性能。而且，朴素
    贝叶斯模型可以作为一个很好的模型测试基准，用于比较其他模型的性能

评估分类模型的性能
-------------------------------
1.预测的正确率和错误率
    正确率等于训练样本中被正确分类的数目除以总样本数: 正确数量/样本总数
    错误率等于训练样本中被错误分类的样本数目除以总样本数：错误数量/样本总数
2.准确率通常用于评价结果的质量，而召回率用来评价结果的完整性
    准确率:预测正确的数量/预测为正确的数量
    召回率:预测正确的数量/真实正确的数量
准确率和召回率是负相关的，高准确率常常对应低召回率，反之亦然
准确率和召回率在单独度量时用处不大，但是它们通常会被一起组成聚合或者平均度量。
二者同时也依赖于模型中选择的阈值。


模型改进
------------------------------------------
1.特征化
2.其他类别特征(多使用一些特征)
3.使用正确的数据模式


模型参数调优
-----------------------------------------------------------------------
逻辑回归和SVM模型有相同的参数，原因是它们都使用随机梯度下降（SGD）作为基础优化
技术。不同点在于二者采用的损失函数不同
==================================

class LogisticRegressionWithSGD private (

//在SGD中，在训练每个样本并更新模型的权重向量时，步长用来控制算法在最陡的梯度方向
  上应该前进多远。较大的步长收敛较快，但是步长太大可能导致收敛到局部最优解
private var stepSize: Double,

//大多数机器学习的方法需要迭代训练，并且经过一定次数的迭代之后收敛到某个解
（即最小化损失函数时的最优权重向量）。 SGD收敛到合适的解需要迭代的次数相对较少，但是要进一步
  提升性能则需要更多次迭代,但提升的也不高
private var numIterations: Int,


//正则化通过限制模型的复杂度避免模型在训练数据中过拟合。
  正则化的具体做法是在损失函数中添加一项关于模型权重向量的函数，从而会使损失增加。
  正则化在现实中几乎是必须的，当特征维度高于训练样本时（此时变量相关需要学习的权重数量
  也非常大）尤其重要。当正则化不存在或者非常低时，模型容易过拟合。
  而且大多数模型在没有正则化的情况会在训练数据上过拟合
  虽然正则化可以得到一个简单模型，但正则化太高可能导致模型欠拟合，从而使模型性能变得很糟糕

MLlib中可用的正则化形式有如下几个。
 SimpleUpdater：相当于没有正则化，是逻辑回归的默认配置。
 SquaredL2Updater：这个正则项基于权重向量的L2正则化，是SVM模型的默认值。
 L1Updater：这个正则项基于权重向量的L1正则化，会导致得到一个稀疏的权重向量（不重要的权重的值接近0)
private var regParam: Double,


//每次迭代时参与计算的样本比例,默认是1(不太清楚用法)
private var miniBatchFraction: Double)


决策树
注意决策树通常不需要特征的标准化和归一化，也不要求将类型特征进行二元编码
===================================
    // 构建模型
    val dt = new DecisionTreeClassifier()
      .setLabelCol("indexedLabel")
      .setFeaturesCol("indexedFeatures")

      //决策树模型选择以下两种不纯度度量方式（常用这两种）： Gini或者Entropy
      //从一个数据集中随机选取子项，度量其被错误的划分到其他组里的概率(还是不太懂)
      .setImpurity("entropy")

      .setMaxBins(100) //离散化"连续特征"的最大划分数

      //提高树的深度可以得到更精确的模型（这和预期一致，因为模型在更大
        的树深度下会变得更加复杂）。然而树的深度越大，模型对训练数据过拟合程度越严重
      .setMaxDepth(5)

      // 一个节点分裂的最小信息增益，值为[0,1](这个不太明白)
      .setMinInfoGain(0.01)

      // 每个节点包含的最小样本数,这个不太明白
      .setMinInstancesPerNode(10)

      .setSeed(123456)



贝叶斯模型
============================================
控制平滑参数


交叉验证技术
--------------------------------------------------------
交叉验证是实际机器学习中的关键部分，同时在多模型选择和参数调优中占有中心地位