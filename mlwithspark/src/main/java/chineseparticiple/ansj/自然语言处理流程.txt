sparkml 没有提供分词工具

ansj 是中科院ictclas的java实现
参考手册: http://nlpchina.github.io/ansj_seg/

word分词器
word分词是一个Java实现的分布式的中文分词组件，提供了多种基于词典的分词算法，并利用ngram模型来消除
歧义。能准确识别英文、数字，以及日期、时间等数量词，能识别人名、地名、组织机构名等未登录词。能通过
自定义配置文件来改变组件行为，能自定义用户词库、自动检测词库变化、支持大规模分布式环境，能灵活指定
多种分词算法，能使用refine功能灵活控制分词结果，还能使用词性标注、同义标注、反义标注、拼音标注等功
能。同时还无缝和Lucene、Solr、ElasticSearch、Luke集成。注意：word1.3需要JDK1.8

其他的参考这个
https://www.cnblogs.com/Sun-godless/p/5629046.html

相应分词器的排名
https://my.oschina.net/apdplat/blog/308391


1.了解应用领域的相关知识
2.创建目标数据集:选择数据
3.数据清理和预处理:(这个可能要占全过程60%的工作量)
4.数据缩减和变换
5.从数据中提取有用特征
  a.数值特征，直接输入的数字值应该进行映射，直接使用有可能不太方便
  b.类别特征，将类别特征表示数字形式时，可以借助k之1(1-of-k)方法进行
  c.派生特征，为了更好的进行模型的训练，有时可以从现有的数据中派生出新的特征
	这些新的特征可以是从现有数据统计出统计类特征，也可以是根据业务场景制定出的特征
  d.文本特征：使用自然语言处理的技术
      下面是一个简单且标准化的文本特征提取方法：
			分词
			删除停用词
			提取词干
			向量化，nk 法
  e.正则化特征
	背后的思想是将各个数值特征进行转换，将它们的值域规范到一个标准区间内
	
  f.使用现有的工具提取特征
     scikit-learn,gensim、 scikit-image、 matplotlib、 Python的NLTK、 
	 Java编写的OpenNLP以及用Scala编写的Breeze和Chalk。实际上， Breeze自Spark 1.0开始就成为Spark的一部分了

6.应用相应的模型进行训练


------------------------------------------------------------------------------------
高级文本特征技术
1.tf-idf
   tf:tf(t,d)是单词t在文档d中的频率（出现的次数）
   idf:log(文档总数/出现过单词t的文档数量)
   TF-IDF公式的含义是：在一个文档中出现次数很多的词相比出现次数少的词应该在词向量表
   示中得到更高的权值。而IDF归一化起到了减弱在所有文档中总是出现的词的作用。最后的结果
   就是，稀有的或者重要的词被给予了更高的权值，而更加常用的单词（被认为比较不重要）则在
   考虑权重的时候有较小的影响
   
2.特征哈希, HashingTF
    接收词条的集合然后把这些集合转化成固定长度的特征向量。这个算法在哈希的同时会统计各个词条的词频
	特征哈希是一种处理高维数据的技术，并经常被应用在文本和分类数据集上，这些数据集的
    特征可以取很多不同的值（经常是好几百万个值）
	可以理解为是升级后的kn法,特征哈希的优势在于不再需要构建映射并把它保存在内存中
	
3.word2vec 模型
    称为分布向量表示的模型
	模型是基于某种文本中与单词共现相关的统计量来构造。一旦向量表示算出，就可以像使用tf-idf向量一样使用
	这些模型。一个比较通用的例子是使用单词的向量表示基于单词的含义计算两个单词的相似度

StringIndexer

IndexerString
    